# Week 2 response brainstorm

-   Human dignity isn't likely to be compromised by the mere fact that our achievements can be matched or outdone
-   is human dignity any worse for that?
-   "It performs a lot of tasks pretty well, rather than performing a single task very well." (gpt3 link)
-   "A better approach is to ask: what is the purpose of using a given AI tool to solve a given set of tasks? How does using AI in a given domain shift, or reify, power in society? Would redefining the problem space itself, rather than optimizing for decision quality, get us closer to justice?" (gpt3 link)

# Week 3 response brainstorm

### Main idea

-   Because AI is developed by humans and feeds off of data from our past, its guaranteed to have some sort of human bias which is determinantal to important decision-making tasks such as risk models in policing, job applications, credit loans, and more
-   Talk about:
    -   Why this is a problem?
    -   What I think we should do moving forward?
        -   Remove variables (talk about why this is also might be an issue)
        -   Process data beforehand (verify in another domain where sensitive attributes were changed if bias took effect)
    -   Is this something to be of blame on the developers?
    -   Loop back to transparency

### Resources

-   "It often means that the algorithms we produce are fueled by aggregating the same intuitive and sometimes prejudiced human decision-making we’re trying to improve on" (Zerelli 48).
-   "As of 2019, major tech companies including Google, Facebook, and Microsoft, have all announced their intention to develop tools for bias detection, although it’s notable that these are all “in-house.”"
-   "Of course, machine learning systems are doing something much more complex than averaging. Nevertheless, fed biased data, they’ll produce biased results" (Zerelli 49).
-   "At Primer, Dr. Bohannon and his engineers recently used BERT to build a system that lets businesses automatically judge the sentiment of headlines, tweets and other streams of online media. Businesses use such tools to inform stock trades and other pointed decisions. But after training his tool, Dr. Bohannon noticed a consistent bias. If a tweet or headline contained the word “Trump,” the tool almost always judged it to be negative, no matter how positive the sentiment." (https://www.nytimes.com/2019/11/11/technology/artificial-intelligence-bias.html)

# Week 4 response brainstorm

### Main idea 1

-   Since complex systems like autonomous technologies are built by many human beings in order to work well, it's unreasonable to point out which individual is responsible for any mishap, but I think it should be the organization as a whole
-   Talk about:
    -   How teams come together to build these things (should take responsibility as such)
    -   The bigger the system gets, the bigger the pool of human beings there are to take responsibility

### Resources 1

-   "In short, for an autonomous technology to work, many human beings have to make many decisions, and these all involve choices for which they could be held responsible" (Zerelli 71).
-   "Either way, it's important that manufacturers be held responsible for the harms caused by their designs, otherwise they’ll have little incentive to improve them" (Zerelli 75-76).

### Main idea 2

-   Just because some automated systems are complex, it does not mean that we can hand wave the fact that humans get complacent with them
-   Talk about:
    -   We should shift our worries from the idea of overtrust but instead to the idea of who we let supervise these systems
    -   No matter how complex the system, we either need intelligently trained individuals and/or teams to be present and know the limitations and potential risks associated with the system so that appropriate action can be taken when necessary
    -   Need regular testing and monitoring of systems
    -   Also need a healthy balance of trust and distrust

### Resources 2

-   "Put simply, once humans are accustomed to trust a system that’s reliable most of the time (but not all of the time), humans themselves tend to “switch off,” falling into a sort of “autopilot” mode where diffidence, complacency and overtrust set in" (Zerelli 80).
-   " If you had complete trust in that AI, you would probably never check its work. If that were the case, and for some reason the AI suddenly did make a mistake, there’s a good chance you would miss the mistake. However, the solution isn’t to never trust the AI either. If you do not trust the AI at all then you would have to check all of its work, which would be an inefficient use of time and resources" (https://blogs.dal.ca/openthink/trust-in-an-ai-neither-over-nor-under-trusting/)
-   "In early 2019, Google AI researchers working with Northwestern Medicine created an AI model capable of detecting lung cancer from screening tests better than human radiologists with an average eight years of experience" (https://venturebeat.com/ai/confidence-uncertainty-and-trust-in-ai-affect-how-humans-make-decisions/)
-   "Around the same time, MIT CSAIL and Massachusetts General Hospital published a paper in the journal Radiology about a system capable of predicting the onset of breast cancer five years in advance using mammography exam imagery" (https://venturebeat.com/ai/confidence-uncertainty-and-trust-in-ai-affect-how-humans-make-decisions/)

# Week 5 response brainstorm

### Main idea

-   Consent is usually required but the way it was obtained has gotten increasingly more transparent over the years

    -   Apple's privacy feature on apps

-   Final note:
    -   Privacy concerns can also depend on the person and what their values are
    -   For everyone to care about privacy is unreasonable as some people are just okay with getting targeted ads on their favourite social media site or recommendations on their Amazon account. They just care about having their needs met with these technologies
    -   Some people will disconnect as much as they can and use software's that adhere to privacy and open'ness (take open sourced software for example)
    -   Point is that everyone can put any variable amounts of effort into privacy and so the best thing to do is give the choice
    -   Also, it would be unreasonable to say that the data collected on you with or without consent have truly imposed a negative outcome in your life.
        -   At least for me, (talk about github copilot and shit)
        -   for everyone else, customer satisfaction and shit

### Resources

-   "However, the same survey found that nearly two thirds of people were not confident that their online activities would be kept secure by online advertisers, social media sites, search engine providers, or online video sites" (Zerelli 105)
-   "AI enabled chatbots are becoming increasingly popular due to their positive impact in many types of industries. These chatbots have the ability to begin conversations with customers, providing relevant answers to their questions, and can help with every touch point through the whole customer life cycle and purchasing experience" (https://www.forbes.com/sites/cognitiveworld/2020/05/07/how-are-companies-using-ai-to-enhance-the-customer-experience/?sh=67f126d04095)
-   "First introduced in iOS 14.5 last year, the privacy feature gives more control to iPhone users on whether or not they want to be tracked by advertisers online across apps. Prompts by ATT ask users if they would like to be tracked while opening an app. If the permission is denied, the app developer can no longer access Apple’s Identifier For Advertisers (IDFA), a device ID that is used to target and measure the effectiveness of online ads. IDFA has traditionally been used to track consumer behaviour for personalised advertising" (https://www.outlookindia.com/business/apple-vs-facebook-how-a-small-change-in-apple-iphone-s-privacy-feature-is-threatening-business-model-of-social-media-companies-news-236802)
-   "This has created a paradoxical situation in which, in order to access the internet and enjoy their human rights online, people are forced to submit to a system predicated on interference with the right to privacy on an unprecedented scale, with corresponding impacts on a range of other human rights, including the right to freedom of expression and non-discrimination" (Amnesty 2019 - Surveillance Giants 40).

# Week 8 response brainstorm

### Main idea

-   Why were audio/video perceptual evidence in the first place, and what can we do to replace it?
    -   Examples of things we have always looked twice about are news stories, online articles, digital images, and now video recordings
-   Connect to how photoshopped images of people and altercations similar to it, always go through a wall of skepticism from the public

### Resources

-   "But I think that the most important risk is not that deepfakes will be believed, but instead that increasingly savvy information consumers will come to reflexively distrust all recordings" (Rini 7).
-   "In effect, recordings will be demoted from sources of perceptual evidence to sources of mere testimonial evidence. And if they are simply just another source of testimony, then they cannot be relied upon to correct or regulate testimonial practice" (Rini 10).

# Week 9 response brainstorm

### Main idea

-   Robots and Animals are not the same
    -   Animals are natural beings like us that do incur feelings, but robots are man made and are entities that are not really alive, they merely act that way but do not have a sense of self since they are just fed human like features
-   something something organic matter
-   counter the argument about how biological parts of someone was gradually replaced time and time again at what point will it lose moral status

### Resources

-   The manufacturers of robots will get their creations to mimic certain behavioural cues that we associate with beings with significant moral status, but because of the internal nature of the robots, these behavioural cues will not correlate with (or supervene upon) the metaphysical properties that we think ground moral status (Danaher pg13).

# Week 10 response brainstorm

### Main idea

-   Connect to continuity thesis and how this was said before but this is what happened

-   Conclude with this quote:
    -   "Robots may have a smaller margin of technical error, but they lack your humanity, your quirks, and the rare and distinctive qualities that make you who you are" (https://hbr.org/2021/03/why-robots-wont-steal-your-job).

### Resources

-   "The fact that the typical bank clerk’s job description today might only faintly resemble what it was in, say, 1980, doesn’t mean there are no more bank clerks. More gener- ally, automation has a way of redrawing boundaries around traditional job categories" (Zerelli pg151).

# Week 11 response brainstorm

### Main idea

-   chatgpt'd basically

### Resources

-   "Moreover, we should note that whereas a pragmatic argument like the one Hevelke and Nida‐Rümelin present against holding car manufacturers responsible is certainly important to consider, it does not settle the question of whether it is just or fair to hold car manufacturers responsible for harms or deaths their cars might cause" (Nyholm 3).

# Week 12 response brainstorm

### Main idea

-   chatgpt'd basically

### Resources

# Week 13 response brainstorm

### Main idea

-   chatgpt'd basically

### Resources
- ""
- "AI labs and independent experts should use this pause to jointly develop and implement a set of shared safety protocols for advanced AI design and development that are rigorously audited and overseen by independent outside experts. These protocols should ensure that systems adhering to them are safe beyond a reasonable doubt" (https://futureoflife.org/open-letter/pause-giant-ai-experiments/).
